{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact code copied from here - https://github.com/wayne391/Lead-Sheet-Dataset/blob/master/src/theorytab_crawler.py  \n",
    "Imported to be able to run from notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "website = 'https://www.hooktheory.com'\n",
    "base_url = website + '/theorytab/artists/'\n",
    "sleep_time = 0.11\n",
    "alphabet_list = string.ascii_lowercase\n",
    "# alphabet_list = 'x'\n",
    "# root_dir = '../datasets'\n",
    "# root_xml = '../datasets/xml'\n",
    "root_dir = '../data/midi/hooktheory'\n",
    "root_xml = '../data/midi/hooktheory/xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_retrieval(artist, song, path_song):\n",
    "\n",
    "    suffix = '/theorytab/view/' + artist + '/' + song\n",
    "    song_url = song_url = 'https://www.hooktheory.com' + suffix\n",
    "    response_song = requests.get(song_url)\n",
    "\n",
    "    soup = BeautifulSoup(response_song.text, 'html.parser')\n",
    "\n",
    "    section_list = [item['href'].split('#')[-1] for item in soup.find_all('a', {'href': re.compile(suffix+'#')})]\n",
    "    pk_list = [item['href'].split('/')[-1] for item in soup.find_all('a', {'href': re.compile(\"/theorytab/chords/pk/\")})]\n",
    "\n",
    "    # save xml\n",
    "    for idx, pk in enumerate(pk_list):\n",
    "        req_url = 'https://www.hooktheory.com/songs/getXmlByPk?pk=' + str(pk)\n",
    "        response_info = requests.get(req_url)\n",
    "        content = response_info.text\n",
    "\n",
    "        with open(os.path.join(path_song, section_list[idx] + \".xml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        time.sleep(0.08)\n",
    "\n",
    "    # get genre\n",
    "    wikiid = soup.findAll(\"multiselect\", {\"items\": \"genres\"})[0]['wikiid']\n",
    "    response_genre = requests.get('https://www.hooktheory.com/wiki/' + str(wikiid) + '/genres')\n",
    "    genre_act_list = json.loads(response_genre.text)\n",
    "    genres = []\n",
    "    for g in genre_act_list:\n",
    "        if g['active']:\n",
    "            genres.append(g['name'])\n",
    "\n",
    "    # saving\n",
    "    info = {'section': section_list, 'pk': pk_list, 'song_url': song_url,\n",
    "            'genres': genres, 'wikiid': wikiid}\n",
    "\n",
    "    with open(os.path.join(path_song, 'song_info.json'), \"w\") as f:\n",
    "        json.dump(info, f)\n",
    "\n",
    "\n",
    "def get_song_list(url_artist, quite=False):\n",
    "    response_tmp = requests.get(website + url_artist)\n",
    "    soup = BeautifulSoup(response_tmp.text, 'html.parser')\n",
    "    item_list = soup.find_all(\"li\", {\"class\": re.compile(\"overlay-trigger\")})\n",
    "\n",
    "    song_name_list = []\n",
    "    for item in item_list:\n",
    "        song_name = item.find_all(\"a\", {\"class\": \"a-no-decoration\"})[0]['href'].split('/')[-1]\n",
    "        song_name_list.append(song_name)\n",
    "        if not quite:\n",
    "            print('   > %s' % song_name)\n",
    "    return song_name_list\n",
    "\n",
    "\n",
    "def traverse_website():\n",
    "    '''\n",
    "    Retrieve all urls of artists and songs from the website\n",
    "    '''\n",
    "\n",
    "    list_pages = []\n",
    "    archive_artist = dict()\n",
    "    artist_count = 0\n",
    "    song_count = 0\n",
    "\n",
    "    for ch in alphabet_list:\n",
    "        time.sleep(sleep_time)\n",
    "        url = base_url + ch\n",
    "        response_tmp = requests.get(url)\n",
    "        soup = BeautifulSoup(response_tmp.text, 'html.parser')\n",
    "        page_count = 0\n",
    "\n",
    "        print('==[%c]=================================================' % ch)\n",
    "\n",
    "        # get artists list by pages\n",
    "        url_artist_list = []\n",
    "        for page in range(1, 9999):\n",
    "            url = 'https://www.hooktheory.com/theorytab/artists/'+ch+'?page=' + str(page)\n",
    "            print(url)\n",
    "            time.sleep(sleep_time)\n",
    "            response_tmp = requests.get(url)\n",
    "            soup = BeautifulSoup(response_tmp.text, 'html.parser')\n",
    "            item_list = soup.find_all(\"li\", {\"class\": re.compile(\"overlay-trigger\")})\n",
    "\n",
    "            if item_list:\n",
    "                page_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            for item in item_list:\n",
    "                url_artist_list.append(item.find_all(\"a\", {\"class\": \"a-no-decoration\"})[0]['href'])\n",
    "\n",
    "        print('Total:', len(url_artist_list))\n",
    "\n",
    "        print('----')\n",
    "\n",
    "        if not page_count:\n",
    "            page_count = 1\n",
    "\n",
    "        # get song of artists\n",
    "        artist_song_dict = dict()\n",
    "\n",
    "        for url_artist in url_artist_list:\n",
    "            artist_count += 1\n",
    "            time.sleep(sleep_time)\n",
    "            artist_name = url_artist.split('/')[-1]\n",
    "            print(artist_name)\n",
    "            song_name_list = get_song_list(url_artist)\n",
    "            song_count += len(song_name_list)\n",
    "            artist_song_dict[artist_name] = song_name_list\n",
    "\n",
    "        archive_artist[ch] = artist_song_dict\n",
    "        list_pages.append(page_count)\n",
    "\n",
    "    print('=======================================================')\n",
    "    print(list_pages)\n",
    "    print('Artists:', artist_count)\n",
    "    print('Songs:', song_count)\n",
    "\n",
    "    archive_artist['num_song'] = song_count\n",
    "    archive_artist['num_artist'] = artist_count\n",
    "\n",
    "    return archive_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_artist = traverse_website()\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "\n",
    "if not os.path.exists(root_xml):\n",
    "    os.makedirs(root_xml)\n",
    "\n",
    "path_artists = os.path.join(root_dir, 'archive_artist.json')\n",
    "with open(path_artists, \"w\") as f:\n",
    "    json.dump(archive_artist, f)\n",
    "\n",
    "with open(path_artists, \"r\") as f:\n",
    "    archive_artist = json.load(f)\n",
    "\n",
    "count_ok = 0\n",
    "song_count = archive_artist['num_song']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in alphabet_list:\n",
    "    path_ch = os.path.join(root_xml, ch)\n",
    "    print('==[%c]=================================================' % ch)\n",
    "\n",
    "    if not os.path.exists(path_ch):\n",
    "        os.makedirs(path_ch)\n",
    "\n",
    "    for a_name in archive_artist[ch].keys():\n",
    "        for s_name in archive_artist[ch][a_name]:\n",
    "\n",
    "            try:\n",
    "                print('(%3d/%3d) %s   %s' % (count_ok, song_count, a_name, s_name))\n",
    "                path_song = os.path.join(path_ch, a_name, s_name)\n",
    "\n",
    "                if not os.path.exists(path_song):\n",
    "                    os.makedirs(path_song)\n",
    "\n",
    "                time.sleep(sleep_time)\n",
    "                song_retrieval(a_name, s_name, path_song)\n",
    "\n",
    "                count_ok += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "print('total:', count_ok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
